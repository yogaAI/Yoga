{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6Dvh-WQrEtpq"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "B6rSi2IoEtpt"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12896\\1938900093.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFmgbBi-Etpt"
   },
   "outputs": [],
   "source": [
    "device = 'cpu' # 학습시 gpu 사용을 결정, (cuda : gpu 사용, cpu : cpu 사용)\n",
    "batch_size = 32 # batch size 결정\n",
    "num_workers = 4 # gpu 사용 시 사용 효율과 관련됨. 무조건 크다고 좋지 않음, system 의존\n",
    "lr = 1e-3 # learning rate\n",
    "momentum = 0.9 # optimizer momentum parameter\n",
    "weight_decay = 1e-5 # weight decay rate\n",
    "EPOCH = 3 # train epoch\n",
    "n_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr5ktkWjEtpu"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "\"\"\"\n",
    "Image Resize & array to tensor & 증강\n",
    "train set에만 적용한다. \n",
    "test는 unseen 이라고 가정되기때문에 어떤 변화든 줄 수 없음.\n",
    "\"\"\"\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "    transforms.Resize((128, 128)), #image resize\n",
    "    transforms.RandomHorizontalFlip(), # image Horizontal Flip \n",
    "    transforms.RandomChoice([\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.2), # color transform\n",
    "        transforms.RandomResizedCrop(128), \n",
    "        transforms.RandomAffine(\n",
    "            degrees=5, translate=(0.5, 0.5),\n",
    "            scale=(0.8, 1.0), shear=5) # random 하게 이미지 변형\n",
    "    ]),\n",
    "    transforms.ToTensor(), # tensor type으로 변경\n",
    "    transforms.Normalize((0.4452, 0.4457, 0.4464), (0.2592, 0.2596, 0.2600)), # image normalize\n",
    "]),\n",
    "    'test': transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4452, 0.4457, 0.4464), (0.2592, 0.2596, 0.2600)),\n",
    "])\n",
    "}\n",
    "\n",
    "# torch datasets 사용하여 image folder로부터 dataset 객체를 생성한다.\n",
    "train_dataset = datasets.ImageFolder('dataset/train', transform=data_transforms['train'])\n",
    "# test_dataset = datasets.ImageFolder('dataset/test', transform=data_transforms['test'])\n",
    "\n",
    "# dataset 에서 data를 어떻게 sampling 할지 결정함.\n",
    "\n",
    "sampler = RandomSampler(train_dataset,\n",
    "                        replacement=True,\n",
    "                        num_samples=batch_size * n_steps)\n",
    "\n",
    "\n",
    "# 실제 학습 시 dataset에서 batch 단위로 갖고오도록 하는 DataLoader 를 생성하여 \n",
    "# 이전에 설정한 dataset, sampler를 인자로 넣어준다.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              sampler=sampler, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                              shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "print(len(train_dataset)) # b , 3, 64, 64\n",
    "# print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENW0uYuuEtpv"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at train dataset\n",
    "# 임의의 index를 부여하여 dataset 내 image가 어떻게 변경이 되었는지 확인한다.\n",
    "print(f\"Size of train set: {len(train_dataset)}\")\n",
    "index = 0\n",
    "x, y = train_dataset[index]\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Label: {y}\")\n",
    "\n",
    "transforms.functional.to_pil_image(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXqtfwkhEtpv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \"\"\"\n",
    "            latent_dim : VAE의 잠재 변수, 원 data를 재구성하기 위한 잠재 정보. \n",
    "                        클수록 복잡도 증가, tuning을 통해 최적값 찾아야함\n",
    "            \n",
    "            Auto Encoder의 구조와 같이 encoder, decoder로 구성되어있음.\n",
    "            encoder 에는 conv layer, Decoder에는 convtranspose 를 사용하여 이미지 차원을 축소하조 다시 원상복귀시키는 구조\n",
    "            \n",
    "        \"\"\"\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 256, 4, 2, 1), #128\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1), # 64\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 1024, 4, 2, 1), # 32\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 2048, 4, 2, 1), # 16\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2048, 2048, 4, 2, 1), # 8\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        n_channels = self.encoder(torch.empty(1, 3, 128,128)).size(-1)\n",
    "        \n",
    "        \n",
    "        self.enc_out = nn.Sequential(\n",
    "            nn.Linear(n_channels, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, latent_dim * 2))\n",
    "        \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "                    nn.Linear(latent_dim, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 1024),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(1024, 2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2048, 2048),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Unflatten(1, (2048, 1, 1)),  # 1x1x2048로 reshape\n",
    "                    # 아래 ConvTransposed2d를 사용하여 img 원 사이즈로 승강 시킨다.\n",
    "                    nn.ConvTranspose2d(2048, 2048, 4, 2, 1), # 2\n",
    "                    nn.ReLU(True),\n",
    "                    nn.ConvTranspose2d(2048, 1024, 4, 2, 1), # 4\n",
    "                    nn.ReLU(True),\n",
    "                    nn.ConvTranspose2d(1024, 512, 4, 2, 1), # 8\n",
    "                    nn.ReLU(True), \n",
    "                    nn.ConvTranspose2d(512, 256, 4, 2, 1), # 16\n",
    "                    nn.ReLU(True),\n",
    "                    nn.ConvTranspose2d(256, 128, 4, 2, 1), # 32\n",
    "                    nn.ReLU(True),\n",
    "                    nn.ConvTranspose2d(128, 64, 4, 2, 1), # 64\n",
    "                    nn.ReLU(True),\n",
    "                    nn.ConvTranspose2d(64, 3, 4, 2, 1), # 128\n",
    "                    nn.Sigmoid()\n",
    "                )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_enc = self.encoder(x)\n",
    "        mu, logvar = self.enc_out(x_enc).chunk(2, dim=1) # enc network 출력을 두개 tensor로 나눈다. \n",
    "        z = self.reparameterize(mu, logvar) # 위에서 구한 mu, logvar 화률 분포를 토대로 sampling된 잠재변수 z\n",
    "        x_recon = self.decoder(z) # \n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "# 모델 학습\n",
    "# latent_dim 40으로 설정\n",
    "model = VAE(latent_dim=40).to(device)\n",
    "# optimizer 생성\n",
    "# Adam 사용, learning rate, weight_decay 등 위에서 설정한 parameter 적용한다.\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# Train scheduler 설정, cosine Annealing 방식으로 learning rate에 주기를 부여한다.\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# EPOCH & step 1 epoch 당 10000개의 step 존재\n",
    "# 500 Step 당 loss 확인,\n",
    "for epoch in range(EPOCH):\n",
    "    total_loss = 0\n",
    "    for step, (x, y) in enumerate(tqdm(train_dataloader), start=1):\n",
    "        model.train()\n",
    "        x= x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, logvar = model(x)\n",
    "        # KL Divergence Loss 구한다. 원본과 생성 이미지 간의 유사도 \n",
    "        loss = torch.mean(torch.pow(x - x_recon, 2)) + torch.mean(torch.exp(logvar) + mu**2 - 1 - logvar)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "        # Summary & Eval\n",
    "        if step % 500 == 0:\n",
    "            print(f\"[Step {step}] train_loss: {loss.detach().squeeze():.2f}\")\n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(EPOCH+1, num_epochs, total_loss / len(train_dataloader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM4eiHisEtpw"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './vae_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ts76dvkdEtpw"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('vae_model.pt'))  # 모델 경로와 파일 이름에 맞게 수정\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_9r9yb8Etpw"
   },
   "outputs": [],
   "source": [
    "# 이상 탐지 수행\n",
    "model.eval()\n",
    "\n",
    "threshold = 0.01  # 이상치 판별 임계값\n",
    "\n",
    "with torch.no_grad():\n",
    "    anomaly_count = 0\n",
    "    for data, _ in test_loader:\n",
    "        data = data.to(device)\n",
    "        x_hat, mean, log_var = model(data)\n",
    "        loss = nn.functional.binary_cross_entropy(x_hat, data, reduction='mean')\n",
    "        if loss > threshold:\n",
    "            anomaly_count += 1\n",
    "\n",
    "print(\"이상치 개수:\", anomaly_count)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
